{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6aecce-90be-46b8-a2b7-c2a3030eacf7",
   "metadata": {},
   "source": [
    "# Problem Set 2: Analyzing pions with GNNs\n",
    "\n",
    "**Software requirements:** See below -- specialized packages include `pytorch geometric`, `networkx`, `glob` (installed via `pip install glob2`). All requirements are installable via `pip` or `uv`. \n",
    "\n",
    "**Datasets:**\n",
    "I recommend creating a folder called something like `data/pset_2` wherever you are working on this problem set. The datasets can be downloaded at this link: https://uwmadison.box.com/s/fd34yleydcj8l5sonmwhxjqz0ivm8xoc \n",
    "\n",
    "**Grading:**\n",
    "This problem set will be graded as a quiz within Canvas.\n",
    "\n",
    "**Deadline:** \n",
    "The Canvas quiz will close by class time (4pm Central Time) on Wednesday, October 22nd, 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff62d684-cacf-4c1c-968d-a5d97c4d4ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ceph/home/rcruzcan/.conda/envs/mlphys_proj2/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/mnt/ceph/home/rcruzcan/.conda/envs/mlphys_proj2/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx # for visualizing graphs\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from deepsnap.batch import Batch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bd8ba7-ed86-426c-bf84-1b4eb402157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WARNING: don't change this cell -- setting these seeds will ensure that your answers are consistent with mine \n",
    "seed = 1234 \n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ddd1d22-5312-4bd2-beb1-ede259a8f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data (multiple files)\n",
    "folder = \"/mnt/ceph/home/rcruzcan/private/courses/MLPhysics/MLPhysics_Course/projects/project2/data/\"\n",
    "pion_files = glob.glob(folder+\"pion_files/*.npy\")\n",
    "pi0_files = glob.glob(folder+\"pi0_files/*.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e6d07-086b-4ae8-b651-755333c653c3",
   "metadata": {},
   "source": [
    "# Pion classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9db4f0ea-a3ba-4e3e-b138-182ee95406c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b51f7b41a63493a8c924aebe8390007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8db66bc1d5c493da59e4fd13c7b95fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pion dataframe has 10,371 events.\n",
      "Pi0 dataframe has 6,551 events.\n"
     ]
    }
   ],
   "source": [
    "df_pion = pd.concat([pd.DataFrame(np.load(file, allow_pickle=True).item()) for file in tqdm(pion_files[:1])]) # restrict number of files to balance classes\n",
    "df_pi0 = pd.concat([pd.DataFrame(np.load(file, allow_pickle=True).item()) for file in tqdm(pi0_files)])\n",
    "\n",
    "print(\"Pion dataframe has {:,} events.\".format(df_pion.shape[0]))\n",
    "print(\"Pi0 dataframe has {:,} events.\".format(df_pi0.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "41476e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'cluster_cell_E', 'cluster_cell_ID', 'trackPt', 'trackD0',\n",
       "       'trackZ0', 'trackEta_EMB2', 'trackPhi_EMB2', 'trackEta_EME2',\n",
       "       'trackPhi_EME2', 'trackEta', 'trackPhi', 'nCluster', 'nTrack',\n",
       "       'truthPartE', 'truthPartPt', 'cluster_ENG_CALIB_TOT', 'cluster_E',\n",
       "       'cluster_Eta', 'cluster_Phi', 'cluster_EM_PROBABILITY',\n",
       "       'cluster_E_LCCalib', 'cluster_HAD_WEIGHT', 'deltaR', 'dR_pass',\n",
       "       'event_number'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd735054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cluster_cell_E', 'cluster_cell_ID', 'trackPt', 'trackD0', 'trackZ0',\n",
       "       'trackEta_EMB2', 'trackPhi_EMB2', 'trackEta', 'trackPhi', 'nCluster',\n",
       "       'nTrack', 'truthPartE', 'truthPartPt', 'cluster_ENG_CALIB_TOT',\n",
       "       'cluster_E', 'cluster_Eta', 'cluster_Phi', 'cluster_EM_PROBABILITY',\n",
       "       'cluster_E_LCCalib', 'cluster_HAD_WEIGHT', 'dR', 'dR_pass',\n",
       "       'event_number'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pi0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "567110b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cluster_cell_E</th>\n",
       "      <th>cluster_cell_ID</th>\n",
       "      <th>trackPt</th>\n",
       "      <th>trackD0</th>\n",
       "      <th>trackZ0</th>\n",
       "      <th>trackEta_EMB2</th>\n",
       "      <th>trackPhi_EMB2</th>\n",
       "      <th>trackEta_EME2</th>\n",
       "      <th>trackPhi_EME2</th>\n",
       "      <th>...</th>\n",
       "      <th>cluster_ENG_CALIB_TOT</th>\n",
       "      <th>cluster_E</th>\n",
       "      <th>cluster_Eta</th>\n",
       "      <th>cluster_Phi</th>\n",
       "      <th>cluster_EM_PROBABILITY</th>\n",
       "      <th>cluster_E_LCCalib</th>\n",
       "      <th>cluster_HAD_WEIGHT</th>\n",
       "      <th>deltaR</th>\n",
       "      <th>dR_pass</th>\n",
       "      <th>event_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>((7.7120857, 0.017031768, 0.8613132, 0.0408380...</td>\n",
       "      <td>((1149781504, 1149765120, 1149797888, 11497812...</td>\n",
       "      <td>[35.268047]</td>\n",
       "      <td>[0.009745013]</td>\n",
       "      <td>[73.633]</td>\n",
       "      <td>[0.69286096]</td>\n",
       "      <td>[-2.970729]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>...</td>\n",
       "      <td>[27.174538, 0.29033858]</td>\n",
       "      <td>[22.896261, 0.40051812]</td>\n",
       "      <td>[0.6958733, 0.43738556]</td>\n",
       "      <td>[-2.981098, -2.8864744]</td>\n",
       "      <td>[0.00030118643, 0.019664463]</td>\n",
       "      <td>[30.707035, 0.76434183]</td>\n",
       "      <td>[1.152908, 1.0502272]</td>\n",
       "      <td>[0.010472925819019208, 0.27609453465122735]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>((1.9225851, 0.4782984, 1.0708888, 0.21156694,...</td>\n",
       "      <td>((767585972, 767585970, 767585974, 767585460, ...</td>\n",
       "      <td>[28.130478]</td>\n",
       "      <td>[0.0010360059]</td>\n",
       "      <td>[102.36897]</td>\n",
       "      <td>[1.3765892]</td>\n",
       "      <td>[2.274305]</td>\n",
       "      <td>[1.3765893]</td>\n",
       "      <td>[2.2724702]</td>\n",
       "      <td>...</td>\n",
       "      <td>[9.645189, 4.1669793, 7.294774, 3.8841047, 2.7...</td>\n",
       "      <td>[9.111117, 6.341108, 5.3945174, 3.2805374, 1.8...</td>\n",
       "      <td>[1.3920547, 1.4241304, 1.3460875, 1.3462025, 1...</td>\n",
       "      <td>[2.2259603, 2.304043, 2.2316482, 2.3136475, 2....</td>\n",
       "      <td>[0.19883664, 0.2674778, 0.0020222887, 0.013409...</td>\n",
       "      <td>[12.781342, 10.993462, 10.308264, 6.1097336, 6...</td>\n",
       "      <td>[1.0651466, 1.0251846, 1.240434, 1.0657715, 1....</td>\n",
       "      <td>[0.03441024025099552, 0.06707009514837796, 0.0...</td>\n",
       "      <td>[True, True, True, True, True, True]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>((16.31489, 1.3621913, 1.3081299, 0.17543113, ...</td>\n",
       "      <td>((1149732608, 1149716224, 1149732352, 11497328...</td>\n",
       "      <td>[74.77584]</td>\n",
       "      <td>[-0.012955406]</td>\n",
       "      <td>[-8.59665]</td>\n",
       "      <td>[0.7014117]</td>\n",
       "      <td>[2.9576569]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>...</td>\n",
       "      <td>[49.449574, 26.568502, 0.24031131, 0.25694823]</td>\n",
       "      <td>[42.85047, 27.031446, 0.8118878, 0.2950738]</td>\n",
       "      <td>[0.7013916, 0.6824008, 0.78855264, 0.8235522]</td>\n",
       "      <td>[2.980659, 2.9671967, 2.7977874, 3.128827]</td>\n",
       "      <td>[0.00038892106, 0.041169945, 0.017027207, 0.02...</td>\n",
       "      <td>[53.96393, 36.372078, 0.8447713, 0.7373382]</td>\n",
       "      <td>[1.1539516, 1.0616913, 1.0, 1.2007517]</td>\n",
       "      <td>[0.013336900928857417, 0.01900154013426954, 0....</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>((6.8593082, 3.1947296, 0.45396918, 0.6548578,...</td>\n",
       "      <td>((767561556, 767561554, 767561558, 767561044, ...</td>\n",
       "      <td>[64.417206]</td>\n",
       "      <td>[-0.025962753]</td>\n",
       "      <td>[118.277374]</td>\n",
       "      <td>[0.12484062]</td>\n",
       "      <td>[-2.106485]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>...</td>\n",
       "      <td>[49.988007]</td>\n",
       "      <td>[47.388824]</td>\n",
       "      <td>[0.18143325]</td>\n",
       "      <td>[-2.1094959]</td>\n",
       "      <td>[0.003837438]</td>\n",
       "      <td>[63.956497]</td>\n",
       "      <td>[1.1195381]</td>\n",
       "      <td>[0.05722628251570102]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>((0.9065694, 0.04256741, 0.6076689, 0.03359242...</td>\n",
       "      <td>((1141014528, 1140998144, 1141030912, 11410147...</td>\n",
       "      <td>[10.022616]</td>\n",
       "      <td>[-0.0033622894]</td>\n",
       "      <td>[-35.850327]</td>\n",
       "      <td>[0.03406541]</td>\n",
       "      <td>[1.0819385]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>[-1e+09]</td>\n",
       "      <td>...</td>\n",
       "      <td>[2.96507, 3.6390934]</td>\n",
       "      <td>[3.5169182, 3.2089727]</td>\n",
       "      <td>[-0.0043394375, 0.026746975]</td>\n",
       "      <td>[1.0550588, 1.0636826]</td>\n",
       "      <td>[0.0, 0.020064265]</td>\n",
       "      <td>[3.5169182, 5.518128]</td>\n",
       "      <td>[1.0, 1.1769588]</td>\n",
       "      <td>[0.0612177658530893, 0.05681867182434177]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                     cluster_cell_E  \\\n",
       "0      1  ((7.7120857, 0.017031768, 0.8613132, 0.0408380...   \n",
       "1      4  ((1.9225851, 0.4782984, 1.0708888, 0.21156694,...   \n",
       "2      5  ((16.31489, 1.3621913, 1.3081299, 0.17543113, ...   \n",
       "3      6  ((6.8593082, 3.1947296, 0.45396918, 0.6548578,...   \n",
       "4     11  ((0.9065694, 0.04256741, 0.6076689, 0.03359242...   \n",
       "\n",
       "                                     cluster_cell_ID      trackPt  \\\n",
       "0  ((1149781504, 1149765120, 1149797888, 11497812...  [35.268047]   \n",
       "1  ((767585972, 767585970, 767585974, 767585460, ...  [28.130478]   \n",
       "2  ((1149732608, 1149716224, 1149732352, 11497328...   [74.77584]   \n",
       "3  ((767561556, 767561554, 767561558, 767561044, ...  [64.417206]   \n",
       "4  ((1141014528, 1140998144, 1141030912, 11410147...  [10.022616]   \n",
       "\n",
       "           trackD0       trackZ0 trackEta_EMB2 trackPhi_EMB2 trackEta_EME2  \\\n",
       "0    [0.009745013]      [73.633]  [0.69286096]   [-2.970729]      [-1e+09]   \n",
       "1   [0.0010360059]   [102.36897]   [1.3765892]    [2.274305]   [1.3765893]   \n",
       "2   [-0.012955406]    [-8.59665]   [0.7014117]   [2.9576569]      [-1e+09]   \n",
       "3   [-0.025962753]  [118.277374]  [0.12484062]   [-2.106485]      [-1e+09]   \n",
       "4  [-0.0033622894]  [-35.850327]  [0.03406541]   [1.0819385]      [-1e+09]   \n",
       "\n",
       "  trackPhi_EME2  ...                              cluster_ENG_CALIB_TOT  \\\n",
       "0      [-1e+09]  ...                            [27.174538, 0.29033858]   \n",
       "1   [2.2724702]  ...  [9.645189, 4.1669793, 7.294774, 3.8841047, 2.7...   \n",
       "2      [-1e+09]  ...     [49.449574, 26.568502, 0.24031131, 0.25694823]   \n",
       "3      [-1e+09]  ...                                        [49.988007]   \n",
       "4      [-1e+09]  ...                               [2.96507, 3.6390934]   \n",
       "\n",
       "                                           cluster_E  \\\n",
       "0                            [22.896261, 0.40051812]   \n",
       "1  [9.111117, 6.341108, 5.3945174, 3.2805374, 1.8...   \n",
       "2        [42.85047, 27.031446, 0.8118878, 0.2950738]   \n",
       "3                                        [47.388824]   \n",
       "4                             [3.5169182, 3.2089727]   \n",
       "\n",
       "                                         cluster_Eta  \\\n",
       "0                            [0.6958733, 0.43738556]   \n",
       "1  [1.3920547, 1.4241304, 1.3460875, 1.3462025, 1...   \n",
       "2      [0.7013916, 0.6824008, 0.78855264, 0.8235522]   \n",
       "3                                       [0.18143325]   \n",
       "4                       [-0.0043394375, 0.026746975]   \n",
       "\n",
       "                                         cluster_Phi  \\\n",
       "0                            [-2.981098, -2.8864744]   \n",
       "1  [2.2259603, 2.304043, 2.2316482, 2.3136475, 2....   \n",
       "2         [2.980659, 2.9671967, 2.7977874, 3.128827]   \n",
       "3                                       [-2.1094959]   \n",
       "4                             [1.0550588, 1.0636826]   \n",
       "\n",
       "                              cluster_EM_PROBABILITY  \\\n",
       "0                       [0.00030118643, 0.019664463]   \n",
       "1  [0.19883664, 0.2674778, 0.0020222887, 0.013409...   \n",
       "2  [0.00038892106, 0.041169945, 0.017027207, 0.02...   \n",
       "3                                      [0.003837438]   \n",
       "4                                 [0.0, 0.020064265]   \n",
       "\n",
       "                                   cluster_E_LCCalib  \\\n",
       "0                            [30.707035, 0.76434183]   \n",
       "1  [12.781342, 10.993462, 10.308264, 6.1097336, 6...   \n",
       "2        [53.96393, 36.372078, 0.8447713, 0.7373382]   \n",
       "3                                        [63.956497]   \n",
       "4                              [3.5169182, 5.518128]   \n",
       "\n",
       "                                  cluster_HAD_WEIGHT  \\\n",
       "0                              [1.152908, 1.0502272]   \n",
       "1  [1.0651466, 1.0251846, 1.240434, 1.0657715, 1....   \n",
       "2             [1.1539516, 1.0616913, 1.0, 1.2007517]   \n",
       "3                                        [1.1195381]   \n",
       "4                                   [1.0, 1.1769588]   \n",
       "\n",
       "                                              deltaR  \\\n",
       "0        [0.010472925819019208, 0.27609453465122735]   \n",
       "1  [0.03441024025099552, 0.06707009514837796, 0.0...   \n",
       "2  [0.013336900928857417, 0.01900154013426954, 0....   \n",
       "3                              [0.05722628251570102]   \n",
       "4          [0.0612177658530893, 0.05681867182434177]   \n",
       "\n",
       "                                dR_pass event_number  \n",
       "0                          [True, True]            1  \n",
       "1  [True, True, True, True, True, True]            4  \n",
       "2              [True, True, True, True]            5  \n",
       "3                                [True]            6  \n",
       "4                          [True, True]           11  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b4ab7a01-6506-4211-be92-c368165f7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df, is_charged=False): \n",
    "    ### Start the dataframe of inputs \n",
    "    max_n_cols = pd.DataFrame(df.cluster_E.to_list()).shape[1]\n",
    "    df2 = pd.DataFrame(pd.DataFrame(df.cluster_E.to_list(), columns=[\"cluster_e_\"+str(x) for x in np.arange(max_n_cols)]))\n",
    "    \n",
    "    df3 = pd.DataFrame(pd.DataFrame(df.cluster_Eta.to_list(), columns=[\"cluster_eta_\"+str(x) for x in np.arange(max_n_cols)]))\n",
    "    df2['cluster_eta_0'] = df3['cluster_eta_0'] \n",
    "    \n",
    "    df3 = pd.DataFrame(pd.DataFrame(df.cluster_Phi.to_list(), columns=[\"cluster_phi_\"+str(x) for x in np.arange(max_n_cols)]))\n",
    "    df2['cluster_phi_0'] = df3['cluster_phi_0']   \n",
    "    \n",
    "    ### Add cluster cell energy\n",
    "    log10_cluster_cell_e = []\n",
    "    for i in range(len(df2)): \n",
    "        log10_cluster_cell_e.append(np.array(np.log10(df.cluster_cell_E.iloc[i][0]))) # only cells from leading cluster\n",
    "    df3[\"log10_cluster_cell_e\"] = log10_cluster_cell_e\n",
    "    max_n_cells = pd.DataFrame(df3.log10_cluster_cell_e.to_list()).shape[1]\n",
    "    df_cells = pd.DataFrame(pd.DataFrame(df3.log10_cluster_cell_e.to_list(), columns=[\"log10_cluster_cell_e_\"+str(x) for x in np.arange(max_n_cells)]))\n",
    "    df2 = pd.concat([df2, df_cells], axis=\"columns\")\n",
    "\n",
    "    ### Leading cluster_E > 0.5\n",
    "    df2 = df2[df2['cluster_e_0'] > 0.5]\n",
    "    \n",
    "    ### Cast as float\n",
    "    df2 = df2.astype('float32')\n",
    "\n",
    "    ### Add the log of leading cluster energy\n",
    "    for var in ['cluster_e_0']:\n",
    "        df2['log10_'+var] = np.log10(df2[var])\n",
    "    \n",
    "    ### Reduce variables\n",
    "    vars = [\n",
    "    'log10_cluster_e_0', \n",
    "    'cluster_eta_0',\n",
    "    'cluster_phi_0',\n",
    "             ]\n",
    "    \n",
    "    vars += [f'log10_cluster_cell_e_{i}' for i in range(10)] ### top 10 cells\n",
    "    \n",
    "    df2 = df2[vars]\n",
    "    \n",
    "    df2['num_cells_lead_cluster'] = np.sum(df2[[var for var in df2.keys() if \"scaled\" not in var and \"cell\" in var]] != 0, axis=1)\n",
    "\n",
    "    if is_charged:\n",
    "        df2['label'] = 1\n",
    "    else:\n",
    "        df2['label'] = 0\n",
    "        \n",
    "    ### Drop infs/NaNs \n",
    "    df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df2 = df2.fillna(0)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d5b1d-4233-47a6-96ce-7675f909acf2",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 1: Cleaning the data</span>\n",
    "Add a cut to the dataframe cleaning function to require that the leading cluster energy is $>0.5$ GeV. What is the effect of adding this cut to the number of events in `df_pion`? \n",
    "\n",
    "- a) Reduced by <0.5%\n",
    "- b) Reduced by 0.5%\n",
    "- c) Reduced by 2%\n",
    "- d) Reduced by 6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6ced803-1838-46c9-b7dc-e61a65cadcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pion = clean_dataframe(df_pion, is_charged = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e528fe4-4054-4e98-8ae0-23a9c7567fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pi0 = clean_dataframe(df_pi0, is_charged = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3b5c4-b83c-4ad5-82c9-18db6373fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_pion, df_pi0])\n",
    "df = df.sample(frac=1) # shuffle the rows, for good measure\n",
    "df = df.fillna(0) # fill nans with 0s \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0fb3f-1695-4ca6-8f53-5f1476eb6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PionDataset_Classification(Dataset):\n",
    "    def __init__(self, dataframe, \n",
    "                 cluster_features=['log10_cluster_e_0','cluster_eta_0','cluster_phi_0'], \n",
    "                 transform=None, \n",
    "                 pre_transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.cluster_features = cluster_features\n",
    "        super().__init__(None, transform, pre_transform)\n",
    "        print(f\"Initialized PionDataset with {len(dataframe)} samples\")\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def get(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        dataframe = self.dataframe\n",
    "        \n",
    "        ### Define nodes \n",
    "        cluster_features = self.cluster_features\n",
    "\n",
    "        ### define nodes with topo-cluster CELL energies! \n",
    "        cell_info = ['log10_cluster_cell_e_'+str(i) for i in range(int(dataframe.iloc[index].num_cells_lead_cluster))]\n",
    "        cluster_nodes = np.zeros((int(dataframe.iloc[index].num_cells_lead_cluster), 1+len(cluster_features)))\n",
    "        cluster_nodes[:,0] = dataframe.iloc[index][cell_info]\n",
    "        \n",
    "        cluster_global_node = np.array(dataframe.iloc[index][cluster_features])\n",
    "        cluster_global_node = np.concatenate([np.zeros(1), cluster_global_node]) # cluster features come first\n",
    "        \n",
    "        nodes = np.vstack([cluster_nodes, cluster_global_node]) # shape = (num_nodes, num_node_features)\n",
    "                \n",
    "        ### Define edges (fully-connected, but no self-loops)\n",
    "        edges = [(i, j) for i in range(nodes.shape[0]) for j in range(nodes.shape[0]) if i != j]\n",
    "        edge_index = np.array(edges).T  # Shape: (2, num_edges)\n",
    "        \n",
    "        ### Define target labels \n",
    "        label = np.array([dataframe.iloc[index]['label']])\n",
    "                \n",
    "        ### Convert to torch tensors\n",
    "        nodes = torch.tensor(nodes, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "        return Data(x=nodes, y=label, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487b2f5-e558-442c-ada5-6c4b1a3e6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PionDataset_Classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22fb33-846a-48db-8d3a-c7836d301b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure() \n",
    "graph = dataset[0]\n",
    "if graph.y[0] == 0:\n",
    "    print(\"Neutral pion\")\n",
    "elif graph.y[0] == 1:\n",
    "    print(\"Charged pion\")\n",
    "nx.draw(to_networkx(graph), \n",
    "        cmap='spring', \n",
    "        with_labels=True,\n",
    "        font_weight='bold',\n",
    "        node_color = np.arange(graph.num_nodes),\n",
    "        node_size=200, linewidths=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71ac64-1e2f-48b7-a6d8-84de1b838eec",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 2: Defining a simple graph</span>\n",
    "How many edges are in each graph? Report an integer value for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fc6b6-53c9-4048-aed6-e7b9be6d4851",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 3: Node features</span>\n",
    "How many features are associated with each node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6788c-15de-447c-80fa-fc4d276b0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"hidden_size\" : 64,\n",
    "    \"epochs\" : 10,\n",
    "    \"lr\" : 0.001,\n",
    "    \"num_layers\": 3,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "dataset_train = dataset[:int(0.8*len(dataset))]\n",
    "dataset_val = dataset[int(0.8*len(dataset)):int(0.9*len(dataset))]\n",
    "dataset_test = dataset[int(0.9*len(dataset)):]\n",
    "\n",
    "print(f'Number of training graphs: {len(dataset_train)} ({100*len(dataset_train)/len(dataset):.0f}% of total)')\n",
    "print(f'Number of val graphs: {len(dataset_val)} ({100*len(dataset_val)/len(dataset):.0f}% of total)')\n",
    "print(f'Number of test graphs: {len(dataset_test)} ({100*len(dataset_test)/len(dataset):.0f}% of total)')\n",
    "\n",
    "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(),\\\n",
    "    batch_size=args[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(),\\\n",
    "    batch_size=args[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(),\\\n",
    "    batch_size=args[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79a6e6-f70d-4e5d-9bb4-ba6bec3b77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        ### 1. apply several graph convolutions\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        ### 2. aggregate embeddings across the graph using the \"mean\" aggregation function\n",
    "        x = global_mean_pool(x, batch)  # shape: [batch_size, hidden_channels]\n",
    "\n",
    "        ### 3. apply dropout & get a final graph-level output using a linear layer\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d05f5-c31d-493b-83c0-3c3d202be28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=args['hidden_size'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408c5d6-3045-467f-9376-9b62e36c6a29",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 4: Order of operations during training</span>\n",
    "Put the following operations in order and add them to the training loop below. \n",
    "\n",
    "- a) `loss.backward()`\n",
    "- b) `optimizer.step()`\n",
    "- c) `loss = criterion(out, data.y.float())`\n",
    "- d) `out = model(data.x, data.edge_index, data.batch).squeeze()`\n",
    "- e) `optimizer.zero_grad() `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bb9ca-70a1-4757-abbe-9ccacfe2d9f3",
   "metadata": {},
   "source": [
    "Then, fill out the `test` function to iterate over a generic dataloader and return the average accuracy and loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35446bac-34da-4421-aca1-7b9862fb2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "criterion = torch.nn.BCEWithLogitsLoss() # no need to apply sigmoid activation b/c BCEWithLogitsLoss() has this built in\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        ???\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     correct = 0\n",
    "     loss_ = 0\n",
    "     for data in loader:\n",
    "        ???\n",
    "     return correct / len(loader.dataset), loss_ / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6a9ee-079b-4d2e-922b-04a832d818ef",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 5: Prediction accuracy before training</span>\n",
    "What is the accuracy on the test set *before* training the model? Report your answer rounded to the nearest 10%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7c9f3-3d43-437d-9d16-08e0ee7ae9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "use_wandb = True \n",
    "\n",
    "# Initialize W&B run for training\n",
    "wandb.init(project=\"pset2_classification\") # name your project whatever you like\n",
    "\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train()\n",
    "    train_acc, train_loss = test(train_loader)\n",
    "    val_acc, val_loss = test(val_loader)\n",
    "    \n",
    "    # Log metrics to W&B\n",
    "    if use_wandb:\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss,\n",
    "            \"train/acc\": train_acc,\n",
    "            \"val/acc\": val_acc,\n",
    "            \"val/loss\": val_loss,\n",
    "        })\n",
    "\n",
    "    torch.save(model, \"pion_classification_model.pt\")\n",
    "\n",
    "# Finish the W&B run\n",
    "if use_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a98c7-7b38-4a01-94fb-5e9cc1b23c0d",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 6: Prediction accuracy after training</span>\n",
    "What is the accuracy on the test set *after* training the model? Report your answer rounded to the nearest 5%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d18303-f97f-44d9-816b-8794b0c17cd6",
   "metadata": {},
   "source": [
    "# Pion regression\n",
    "The goal in this section is to correctly predict the truth pion energy based on the cell-level energies for each pion topo-cluster as well as the leading track information. This graph is much more exciting, since it is constructed from as many nodes as there are calorimeter cells with energy deposits > 0.5 GeV! Plus one additional node for the track information. The global feature is the total cluster energy. The regression target is the truth pion energy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92930c3-1c33-4529-a838-d938d04c9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "### we're restricting this to one file just for speed purposes, but there are more files if you want to try training for longer on your own\n",
    "### keep it as n_files = 1 for this problem set \n",
    "n_files = 1 \n",
    "df_pion = pd.concat([pd.DataFrame(np.load(file, allow_pickle=True).item()) for file in tqdm(pion_files[:n_files])])\n",
    "print(\"Pion dataframe has {:,} events.\".format(df_pion.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c7f80-e657-4b78-b0cc-1a2b219c03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df): \n",
    "    ### Start the dataframe of inputs \n",
    "    max_n_cols = pd.DataFrame(df.cluster_E.to_list()).shape[1]\n",
    "    df2 = pd.DataFrame(pd.DataFrame(df.cluster_E.to_list(), columns=[\"cluster_e_\"+str(x) for x in np.arange(max_n_cols)]))\n",
    "    \n",
    "    df3 = pd.DataFrame(pd.DataFrame(df.cluster_Eta.to_list(), columns=[\"cluster_eta_\"+str(x) for x in np.arange(max_n_cols)]))\n",
    "    df2['cluster_eta_0'] = df3['cluster_eta_0'] \n",
    "    \n",
    "    df3 = pd.DataFrame(pd.DataFrame(df.cluster_Phi.to_list(), columns=[\"cluster_phi_\"+str(x) for x in np.arange(max_n_cols)]))\n",
    "    df2['cluster_phi_0'] = df3['cluster_phi_0']   \n",
    "    \n",
    "    ### Add cluster cell energy\n",
    "    log10_cluster_cell_e = []\n",
    "    for i in range(len(df2)): \n",
    "        log10_cluster_cell_e.append(np.array(np.log10(df.cluster_cell_E.iloc[i][0]))) # only cells from leading cluster\n",
    "    df3[\"log10_cluster_cell_e\"] = log10_cluster_cell_e\n",
    "    max_n_cells = pd.DataFrame(df3.log10_cluster_cell_e.to_list()).shape[1]\n",
    "    df_cells = pd.DataFrame(pd.DataFrame(df3.log10_cluster_cell_e.to_list(), columns=[\"log10_cluster_cell_e_\"+str(x) for x in np.arange(max_n_cells)]))\n",
    "    df2 = pd.concat([df2, df_cells], axis=\"columns\")\n",
    "    \n",
    "    ### Add track pT & truth particle E \n",
    "    track_pt = np.array(df.trackPt.explode())\n",
    "    truth_particle_e = np.array(df.truthPartE.explode())\n",
    "    track_eta = np.array(df.trackEta.explode())\n",
    "    track_phi = np.array(df.trackPhi.explode())\n",
    "    track_z0 = np.array(df.trackZ0.explode())\n",
    "\n",
    "    df2[\"track_pt\"] = track_pt\n",
    "    df2[\"track_eta\"] = track_eta\n",
    "    df2[\"track_phi\"] = track_phi\n",
    "    df2[\"track_z0\"] = track_z0\n",
    "    df2[\"truth_particle_e\"] = truth_particle_e\n",
    "        \n",
    "    ### Cluster_E > 0.5\n",
    "    df2 = df2[df2.cluster_e_0 > 0.5]\n",
    "\n",
    "    ### Lose outliers in track pT \n",
    "    df2 = df2[df2.track_pt < 5000]\n",
    "    \n",
    "    ### Cast as float\n",
    "    df2 = df2.astype('float32')\n",
    "\n",
    "    ### Add the log of all energy variables\n",
    "    for var in ['cluster_e_0', 'track_pt', 'truth_particle_e']:\n",
    "        df2['log10_'+var] = np.log10(df2[var])\n",
    "\n",
    "    ### Drop infs/NaNs \n",
    "    df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df2 = df2.fillna(0)\n",
    "    \n",
    "    ### Reduce variables\n",
    "    vars = [\n",
    "    'log10_cluster_e_0', \n",
    "    'log10_track_pt',\n",
    "    'track_eta', \n",
    "    'track_phi',\n",
    "    'track_z0',\n",
    "    'log10_truth_particle_e',\n",
    "    'cluster_eta_0',\n",
    "    'cluster_phi_0',\n",
    "             ]\n",
    "    \n",
    "    vars += [var for var in df2.keys() if \"cell\" in var]\n",
    "    \n",
    "    df2 = df2[vars]\n",
    "    \n",
    "    df2['num_cells_lead_cluster'] = np.sum(df2[[var for var in df2.keys() if \"scaled\" not in var and \"cell\" in var]] != 0, axis=1)\n",
    "\n",
    "    ### Drop infs/NaNs \n",
    "    df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df2 = df2.fillna(0)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44ecfa-7e3b-422c-872b-ed85d0c89836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataframe(df_pion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6b925-1cc9-4eba-a772-8249c682e613",
   "metadata": {},
   "source": [
    "Let's inspect the dataframe we just assembled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4659f-5c68-487a-86b6-b45ac084c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dc590-77db-4445-bdda-fb5c53583a8e",
   "metadata": {},
   "source": [
    "And now we'll convert the dataframe into a graph structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42682ec-33dc-4559-a245-908c445a67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PionDataset_Regression(Dataset):\n",
    "    def __init__(self, dataframe, \n",
    "                 cluster_features=['log10_cluster_e_0'], \n",
    "                 track_features=['log10_track_pt', 'track_eta'],\n",
    "                 transform=None, \n",
    "                 pre_transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.cluster_features = cluster_features\n",
    "        self.track_features = track_features\n",
    "        super().__init__(None, transform, pre_transform)\n",
    "        print(f\"Initialized PionDataset with {len(dataframe)} samples\")\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def get(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        dataframe = self.dataframe\n",
    "        \n",
    "        ### Define nodes \n",
    "        cluster_features = self.cluster_features\n",
    "        track_features = self.track_features\n",
    "\n",
    "        ### define nodes with topo-cluster CELL energies! \n",
    "        cell_info = ['log10_cluster_cell_e_'+str(i) for i in range(int(dataframe.iloc[index].num_cells_lead_cluster))]\n",
    "        cluster_nodes = np.zeros((int(dataframe.iloc[index].num_cells_lead_cluster), 1+len(cluster_features)+len(track_features)))\n",
    "        cluster_nodes[:,0] = dataframe.iloc[index][cell_info]\n",
    "        \n",
    "        cluster_global_node = np.array(dataframe.iloc[index][cluster_features])\n",
    "        cluster_global_node = np.concatenate([np.zeros(1), cluster_global_node, np.zeros(len(track_features))]) # cluster features come first\n",
    "        \n",
    "        track_node = np.array(dataframe.iloc[index][track_features])\n",
    "        track_node = np.concatenate([np.zeros(len(cluster_features)+1), track_node]) # cluster features come first\n",
    "        \n",
    "        nodes = np.vstack([cluster_nodes, cluster_global_node, track_node]) # shape = (num_nodes, num_node_features)\n",
    "                \n",
    "        ### Define edges (fully-connected, but no self-loops)\n",
    "        edges = [(i, j) for i in range(nodes.shape[0]) for j in range(nodes.shape[0]) if i != j]\n",
    "        edge_index = np.array(edges).T  # Shape: (2, num_edges)\n",
    "        \n",
    "        ### Define target labels \n",
    "        target = np.array([dataframe.iloc[index]['log10_truth_particle_e']])\n",
    "                \n",
    "        ### Convert to torch tensors\n",
    "        nodes = torch.tensor(nodes, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        target = torch.tensor(target, dtype=torch.float)\n",
    "        \n",
    "        return Data(x=nodes, y=target, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a379168-d5f8-453d-9735-1a9d255fa04e",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 7: log(E)</span>\n",
    "What is the main reason to take the log of the energy?\n",
    "\n",
    "- a) to compress the dynamic range and make the distribution easier to train on\n",
    "- b) to convert negative values into positive ones\n",
    "- c) to increase the precision of low-energy measurements\n",
    "- d) to satisfy the requirement that neural networks can only process logarithmic inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d2f7b-4191-49ea-b91c-c72a8bb24d56",
   "metadata": {},
   "source": [
    "Create the pion dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc70135-ad01-4e5b-af7e-d79af798af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = ['log10_cluster_e_0']\n",
    "track_features = ['log10_track_pt', 'track_eta', 'track_phi', 'track_z0']\n",
    "dataset = PionDataset_Regression(df, cluster_features, track_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e069703-2bae-4c5a-b2b5-a756919b1793",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 8: Maximum number of nodes</span>\n",
    "What is the maximum number of nodes in any graph in the datasets? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2025a-eea3-4ede-9cdf-723f5a209a57",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 9: Average number of nodes</span>\n",
    "What is the average number of nodes across all the graphs, rounded to the nearest integer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c259f89-8c78-4342-9cc1-a7d7d34f0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \"hidden_size\" : 32,\n",
    "    \"epochs\" : 2, ### keeping the training short\n",
    "    \"lr\" : 0.001,\n",
    "    \"num_layers\": 3,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "dataset_train = dataset[:int(0.8*len(dataset))]\n",
    "dataset_val = dataset[int(0.8*len(dataset)):int(0.9*len(dataset))]\n",
    "dataset_test = dataset[int(0.9*len(dataset)):]\n",
    "\n",
    "print(f'Number of training graphs: {len(dataset_train)} ({100*len(dataset_train)/len(dataset):.0f}% of total)')\n",
    "print(f'Number of val graphs: {len(dataset_val)} ({100*len(dataset_val)/len(dataset):.0f}% of total)')\n",
    "print(f'Number of test graphs: {len(dataset_test)} ({100*len(dataset_test)/len(dataset):.0f}% of total)')\n",
    "\n",
    "train_loader = DataLoader(dataset_train, collate_fn=Batch.collate(),\\\n",
    "    batch_size=args[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, collate_fn=Batch.collate(),\\\n",
    "    batch_size=args[\"batch_size\"])\n",
    "test_loader = DataLoader(dataset_test, collate_fn=Batch.collate(),\\\n",
    "    batch_size=args[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8f740-3ce7-4349-9eb5-ce04c992e862",
   "metadata": {},
   "source": [
    "Using the `args` specified above, define the same GCN architecture that was used above, but make sure your input dimension reflects your new dataset. You should also change your loss function accordingly for the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b60ac-70d2-42e2-bd0f-1d65ef96ddb0",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 10: Regression performance before training</span>\n",
    "What is the MSE of the model before training is performed? Round to the nearest integer value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8090450-86f5-4792-b95e-708dd83371c5",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 11: Regression performance after training</span>\n",
    "What is the RMSE (i.e. $\\sqrt{\\text{MSE}}$) of the model after training is performed over 2 epochs? Round to the nearest integer value and include units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0e384-cc9c-4f54-b604-9df6dd2d4e11",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 12: Performance evaluation</span>\n",
    "\n",
    "Make some plots to evaluate the quality of the trained model. What is true of the performance overall? \n",
    "\n",
    "- a) The model is predicting the same value for every test input. \n",
    "- b) For true pion energies < 200 GeV, the model is generally predicting energy values that are too small. \n",
    "- c) For true pion energies < 200 GeV, the model is generally predicting energy values that are too large. \n",
    "- d) For true pion energies > 200 GeV, the model is generally predicting energy values that are too small. \n",
    "- e) For true pion energies > 200 GeV, the model is generally predicting energy values that are too large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a0496-93a5-4c9f-ae0a-14789d1f532d",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #1f77b4; color: #1f77b4;\">Problem 13 (challenge): Response IQR</span>\n",
    "Define the Interquantile Range (IQR), i.e. the spread between $+1\\sigma$ and $-1\\sigma$, in the function above. We will report 1/2 the IQR divided by the median value using the following reasoning: \n",
    "- What % of the data is expected to fall within $\\pm1\\sigma$ in a Gaussian distribution?\n",
    "- Use `np.percentile` to calculate the values of a generic input $x$ at $\\pm1\\sigma$ from the *median* value of $x$.\n",
    "- Take the difference of these two values to find the IQR.\n",
    "- Divide by 2 to get 1/2 the IQR.\n",
    "- Divide by the median of $x$.\n",
    "\n",
    "Use your custom function as a statistic within `scipy.stats.binned_statistic` applied to: \n",
    "- $\\hat{x}$ axis: True Particle Energy\n",
    "- $\\hat{y}$ axis: Predicted/True Particle Energy\n",
    "- `xbins = [10**exp for exp in np.arange(-1., 3.1, 0.2)]`\n",
    "\n",
    "Make a plot of 1/2 the response IQR divided by the median. Plot each value at the center of its corresponding xbin. What is the value of this quantity for a truth particle energy of 10 GeV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556b9f5f-0724-4f85-bf6a-7278b2d04f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlphys_proj2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
