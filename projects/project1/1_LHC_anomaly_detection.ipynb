{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0175d8-95a1-4919-a7c4-2ef25bf15fc5",
   "metadata": {},
   "source": [
    "# Project 1: LHC anomaly detection challenge\n",
    "\n",
    "**Dataset:** https://uwmadison.box.com/s/702f3xsndlbpc18ud54vemewku63uypg\n",
    "\n",
    "**Grading:**\n",
    "These projects will be peer-graded using the rubric listed below. \n",
    "\n",
    "**Deadline:** \n",
    "The project will be due by class time on Monday, October 6th at 4:00pm Central Time. Please upload a `zip` file to Canvas of the following: \n",
    "- Your completed notebook (`.ipynb` format)\n",
    "- Your completed notebook (`.html` format)\n",
    "- A PDF of an extended abstract, written in LaTeX and no more than 2 pages in length, introducing the problem, summarizing your methods and work, reflecting on your results, listing what future work you would explore, and showing at least one figure. You may use any LaTeX format you like, including 2-column layouts if you prefer. \n",
    "\n",
    "## Project description\n",
    "In this project, you will analyze a dataset consisting of 1 million simulated LHC dijet events for which some number of events correspond to a new, Beyond-the-Standard-Model particle. Below you will find some basic imports and plotting style code as well as instructions on how to load the dataset. Your job is to design an analysis that is able to detect this resonant anomaly in a data-driven manner using weakly-supervised anomaly detection. \n",
    "\n",
    "## Additional resources\n",
    "- https://journals.aps.org/prd/pdf/10.1103/PhysRevD.99.014038\n",
    "- https://arxiv.org/pdf/2101.08320"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb458d0",
   "metadata": {},
   "source": [
    "## Imports & utility function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d4f3e-9d6d-4108-81d4-75a6efe32c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import seaborn as sns\n",
    "import random\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "### ML-related imports\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from livelossplot import PlotLosses\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "### plotting_setup \n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (9,6)\n",
    "plt.rcParams[\"figure.figsize\"] = (7,5)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc(\"font\", size=medium_size)          # default text sizes\n",
    "plt.rc(\"xtick\", labelsize=medium_size)    # xtick labels\n",
    "plt.rc(\"ytick\", labelsize=medium_size)    # ytick labels\n",
    "plt.rc(\"legend\", fontsize=medium_size)    # legend\n",
    "plt.rc(\"axes\", titlesize=large_size)      # axes title\n",
    "plt.rc(\"axes\", labelsize=large_size)      # x and y labels\n",
    "plt.rc(\"figure\", titlesize=large_size)    # figure title\n",
    "\n",
    "SEED = 1234 \n",
    "def set_deterministic(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "SEED = 2001 # for reproducibility \n",
    "set_deterministic(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c328f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mjj(df): \n",
    "    E1 = np.sqrt(df.pxj1**2 + df.pyj1**2 + df.pzj1**2 + df.mj1**2)\n",
    "    E2 = np.sqrt(df.pxj2**2 + df.pyj2**2 + df.pzj2**2 + df.mj2**2)\n",
    "    \n",
    "    Ex = df.pxj1 + df.pxj2\n",
    "    Ey = df.pyj1 + df.pyj2\n",
    "    Ez = df.pzj1 + df.pzj2\n",
    "    E  = E1 + E2\n",
    "    return np.sqrt(np.maximum(E**2 - (Ex**2 + Ey**2 + Ez**2), 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c417137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regions(m0s, sigregion_width, bkgregion_width):\n",
    "    \"\"\"\n",
    "    Generates a list of dictionaries with the signal and background regions for each mass hypothesis\n",
    "    Also checks that the signal regions overlap\n",
    "    \"\"\"\n",
    "    regions = []\n",
    "    all_intervals = []\n",
    "    for m0 in m0s:\n",
    "        sig_low = m0 - sigregion_width / 2\n",
    "        sig_high = m0 + sigregion_width / 2\n",
    "        bkg_left_low = sig_low - bkgregion_width\n",
    "        bkg_left_high = sig_low\n",
    "        bkg_right_low = sig_high\n",
    "        bkg_right_high = sig_high + bkgregion_width\n",
    "\n",
    "        regions.append(\n",
    "            {\n",
    "                \"m0\": m0,\n",
    "                \"sigregion\": (sig_low, sig_high),\n",
    "                \"bkgregion_left\": (bkg_left_low, bkg_left_high),\n",
    "                \"bkgregion_right\": (bkg_right_low, bkg_right_high)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Collect all intervals\n",
    "        all_intervals.extend(\n",
    "            [\n",
    "                (bkg_left_low, bkg_left_high),\n",
    "                (sig_low, sig_high),\n",
    "                (bkg_right_low, bkg_right_high)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if not(regions[0][\"m0\"] >= regions[1][\"m0\"] - sigregion_width):\n",
    "        raise ValueError(\"There are some non-overlapping signal regions!\")\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8af9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(df, input_vars, region, batch_size):\n",
    "    \"\"\"\n",
    "    Generates train, val, test dataloaders and datasets for a given signal and sideband region\n",
    "    \"\"\"\n",
    "    in_any_reg = np.array(df[\"m_jj\"].between(region[\"bkgregion_left\"][0], region[\"bkgregion_right\"][1]))\n",
    "\n",
    "    # Creating separate df and adding label column to new ds\n",
    "    df_in_reg = df[in_any_reg].copy()\n",
    "    df_in_reg[\"labels\"] = df_in_reg[\"m_jj\"].between(region[\"sigregion\"][0], region[\"sigregion\"][1]).astype(int)\n",
    "\n",
    "    # I do it this way to keep track of ordering of events.\n",
    "    # That way I can later plot kinematic distributions of events classified as signal\n",
    "    df_train, df_valtest = train_test_split(df_in_reg, test_size=0.2, random_state=SEED)\n",
    "    df_val, df_test = train_test_split(df_valtest, test_size=0.5, random_state=SEED)\n",
    "\n",
    "    X_train = df_train.filter(items=input_vars).to_numpy(dtype=np.float32)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train = torch.tensor(scaler.transform(X_train))\n",
    "    y_train = torch.tensor(np.array(df_train[\"labels\"], dtype=np.float32).reshape(-1,1))\n",
    "\n",
    "    # Other ds scaled separately\n",
    "    X_val = torch.tensor(scaler.transform(df_val.filter(items=input_vars).to_numpy(dtype=np.float32)))\n",
    "    y_val = torch.tensor(np.array(df_val[\"labels\"], dtype=np.float32).reshape(-1,1))\n",
    "\n",
    "    X_test = torch.tensor(scaler.transform(df_test.filter(items=input_vars).to_numpy(dtype=np.float32)))\n",
    "    y_test = torch.tensor(np.array(df_test[\"labels\"], dtype=np.float32).reshape(-1,1))   \n",
    "    \n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    val_ds = TensorDataset(X_val, y_val)\n",
    "    test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, (dict(train=df_train, val=df_val, test=df_test)), (dict(train=train_ds, val=val_ds, test=test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncorr_vars(corr_df, target_var, threshold, plot=False):\n",
    "    \"\"\"\n",
    "    Identifies variables that are uncorrelated with the target variable. Also can plot the correlations.\n",
    "    \"\"\"\n",
    "    mjj_corr = corr_df[target_var].drop(target_var)\n",
    "    least_correlated = mjj_corr.abs().sort_values()\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        ax.bar(least_correlated.index, least_correlated.values)\n",
    "\n",
    "    return list(least_correlated[least_correlated < threshold].index), (fig, ax) if plot else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d58d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs(logs, region=None):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss curves.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "    ax.plot(logs[\"loss\"], label=\"Training Loss\")\n",
    "    ax.plot(logs[\"val_loss\"], label=\"Validation Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    title = \"Training and Validation Loss\"\n",
    "    if region is not None:\n",
    "        title += f\" ($m_0$ = {region['m0']} GeV)\"\n",
    "    ax.set_title(title)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(dfs, dss, model):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for the given model and dataset.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(dfs[\"test\"][\"labels\"], y_test_preds.cpu().numpy())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fpr, tpr, color=\"darkorange\", lw=2, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"Receiver Operating Characteristic\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907233b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rslts_grids(rslts):\n",
    "    n_cols = 3\n",
    "    n_rows = int(np.ceil(len(rslts) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows), tight_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    for i, rslt in enumerate(rslts):\n",
    "        fpr, tpr, thresholds = rslt[\"roc\"]\n",
    "        ax = axes[i]\n",
    "        ax.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % rslt[\"auc\"])\n",
    "        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(f\"ROC for $m_0$ = {rslt['region']['m0']} GeV\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Make grid of loss plots from rslts. With 3 columns\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows), tight_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    for i, rslt in enumerate(rslts):\n",
    "        ax = axes[i]\n",
    "        ax.plot(rslt[\"logs\"][\"loss\"], label=\"Training Loss\")\n",
    "        ax.plot(rslt[\"logs\"][\"val_loss\"], label=\"Validation Loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"Training and Validation Loss ($m_0$ = {rslt['region']['m0']} GeV)\")\n",
    "        ax.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot model output scores for all mass points on the same plot\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows), tight_layout=True)\n",
    "    axes = axes.flatten()\n",
    "    for i, rslt in enumerate(rslts):\n",
    "        ax = axes[i]\n",
    "        y_test_preds = rslt[\"model\"](rslt[\"X_test_tensor\"].to(device)).detach().cpu().numpy()\n",
    "        ax.hist(y_test_preds, bins=100, range=(0, 1))\n",
    "        ax.set_xlabel(\"Model Output Score\")\n",
    "        ax.set_ylabel(\"Counts\")\n",
    "        ax.set_title(f\"Model Output Scores on Test Set ($m_0$ = {rslt['region']['m0']} GeV)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot AUC vs mass hypothesis\n",
    "    m0s = [rslt[\"region\"][\"m0\"] for rslt in rslts]\n",
    "    aucs = [rslt[\"auc\"] for rslt in rslts]\n",
    "    plt.plot(m0s, aucs, marker='o')\n",
    "    plt.xlabel(r\"$m_0$ [GeV]\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"AUC vs Mass Hypothesis\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0e45c",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6520589-ee32-42ea-a6e8-8877ed9db78f",
   "metadata": {},
   "source": [
    "The dataset consists of 14 variables, for the highest-momentum jet (j1) and the second-highest momentum jet (j2): \n",
    "- The jet mass: $m_j$\n",
    "- The 3-momenta: $p_x$, $p_y$, $p_z$\n",
    "- 3 variables characterizing the jet substructure (\"$n$-subjettiness\"): $\\tau_1$, $\\tau_2$, and $\\tau_3$\n",
    "\n",
    "To the dataframe that holds these variables, we add the invariant mass of the dijets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75933497-37f4-4fe9-8d2c-d0590a73fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"./data/lhc_mystery_anomaly_data.h5\")\n",
    "df[\"m_jj\"] = get_mjj(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e5177",
   "metadata": {},
   "source": [
    "Considering the cylindrical geometry of the ATLAS & CMS experiments at the LHC, we convert the kinematic quantities to more appropriate coordinates.\n",
    "\n",
    "$$\n",
    "    \\eta = \\text{arctanh}\\left(\\frac{\\vec|\\vec p| + p_z}{|\\vec p| - p_z}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\phi = \\text{arctan2}\\left(\\frac{p_y}{px}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p_T = \\sqrt{p_x^2 + p_y^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb29897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ptj1\"] = np.sqrt(df[\"pxj1\"]**2 + df[\"pxj1\"]**2)\n",
    "df[\"ptj2\"] = np.sqrt(df[\"pxj2\"]**2 + df[\"pxj2\"]**2)\n",
    "\n",
    "df[\"phij1\"] = np.arctan2(df[\"pyj1\"], df[\"pxj1\"])\n",
    "df[\"phij2\"] = np.arctan2(df[\"pyj2\"], df[\"pxj2\"])\n",
    "\n",
    "df[\"etaj1\"] = np.arcsinh(df[\"pzj1\"]/df[\"ptj1\"])\n",
    "df[\"etaj2\"] = np.arcsinh(df[\"pzj2\"]/df[\"ptj2\"])\n",
    "\n",
    "# get only columns for jet 1\n",
    "df.filter(like=\"j1\", axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ad9f2",
   "metadata": {},
   "source": [
    "We will also compute the ratio of the n-subjetiness variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "\n",
    "df[\"tau21j1\"] = df[\"tau2j1\"]/df[\"tau1j1\"].replace(0, epsilon)\n",
    "df[\"tau32j1\"] = df[\"tau3j1\"]/df[\"tau2j1\"].replace(0, epsilon)\n",
    "\n",
    "df[\"tau21j2\"] = df[\"tau2j2\"]/df[\"tau1j2\"].replace(0, epsilon)\n",
    "df[\"tau32j2\"] = df[\"tau3j2\"]/df[\"tau2j2\"].replace(0, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68894920",
   "metadata": {},
   "source": [
    "We now plot the given and computed features, as well as the invariant mass, in order to gain some insight into how they are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fad2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"m_jj\"], bins=100, range=(1500, 6500), histtype=\"step\", lw=1)\n",
    "plt.xlabel(r\"$m_{jj}$\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Dijet Mass\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f6ff8",
   "metadata": {},
   "source": [
    "We now visualize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9cfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"mj1\"], bins=100, range=(0, 1000), histtype=\"step\", label=\"Jet 1\", density=True)\n",
    "plt.hist(df[\"mj2\"], bins=100, range=(0, 1000), histtype=\"step\", label=\"Jet 2\", density=True)\n",
    "plt.title(\"Jet Mass Distribution\")\n",
    "plt.xlabel(\"$m_J$ [GeV]\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.phij1, bins=30, range=(-np.pi, np.pi), histtype=\"step\", label=\"Jet 1\", density=True)\n",
    "plt.hist(df.phij2, bins=30, range=(-np.pi, np.pi), histtype=\"step\", label=\"Jet 2\", density=True)\n",
    "plt.title(\"$\\phi$ Distribution of Jets\")\n",
    "plt.xlabel(\"$\\phi$\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d846cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.etaj1, bins=100, range=(-3,3), histtype=\"step\", label=\"Jet 1\", density=True)\n",
    "plt.hist(df.etaj2, bins=100, range=(-3, 3), histtype=\"step\", label=\"Jet 2\", density=True)\n",
    "plt.title(\"$\\eta$ Distribution of Jets\")\n",
    "plt.xlabel(\"$\\eta$\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.ptj1, bins=100, range=(-100, 3500), histtype=\"step\", label=\"Jet 1\", density=True)\n",
    "plt.hist(df.ptj2, bins=100, range=(-100, 3500), histtype=\"step\", label=\"Jet 2\", density=True)\n",
    "plt.title(\"$p_T$ Distribution of Jets\")\n",
    "plt.xlabel(\"$p_T$ [GeV]\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78092088",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    plt.hist(df[f\"tau{i}j1\"], bins=100, range=(0, 1500), histtype=\"step\", label=\"Jet 1\", density=True)\n",
    "    plt.hist(df[f\"tau{i}j2\"], bins=100, range=(0, 1500), histtype=\"step\", label=\"Jet 2\", density=True)\n",
    "    plt.title(f\"$\\\\tau_{i}$ Distribution of Jets\")\n",
    "    plt.xlabel(f\"$\\\\tau_{i}$\")\n",
    "    plt.ylabel(\"A.U.\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53182c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"tau21j1\"], bins=100, range=(0, 1), histtype=\"step\", label=\"Jet 1\", density=True)\n",
    "plt.hist(df[\"tau21j2\"], bins=100, range=(0, 1), histtype=\"step\", label=\"Jet 2\", density=True)\n",
    "plt.title(r\"$\\tau$21 Distribution of Jets\")\n",
    "plt.xlabel(r\"$\\tau$21\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df[\"tau32j1\"], bins=100, range=(0, 1), histtype=\"step\", label=\"Jet 1\", density=True)\n",
    "plt.hist(df[\"tau32j2\"], bins=100, range=(0, 1), histtype=\"step\", label=\"Jet 2\", density=True)\n",
    "plt.title(r\"$\\tau$32 Distribution of Jets\")\n",
    "plt.xlabel(r\"$\\tau$32\")\n",
    "plt.ylabel(\"A.U.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9c422",
   "metadata": {},
   "source": [
    "## Input correlation evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe4703",
   "metadata": {},
   "source": [
    "With the method we will be using, its important that the input features are not significantly correlated with the di-jet mass. Thus, we look at the Pearson correlation between each feature and $m_{jj}$, on which we will apply a cut later on to exclude those features which are too highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(regex=\"^(?!.*(px|py|pz)).*$\") # Remove px, py, pz columns since we will use eta phi and pt\n",
    "cols = [\"m_jj\"] + [col for col in df.columns if \"j1\" in col] + [col for col in df.columns if \"j2\" in col]\n",
    "\n",
    "# We filter out x, y and z variables since we will use eta phi and pt\n",
    "corr_df = df[cols].corr()\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "# Set vmin and vmax to center the colormap around 0\n",
    "sns.heatmap(corr_df, cmap=\"coolwarm\", vmin=-1, vmax=1, fmt=\".2f\", linewidths=0.5, annot=True, annot_kws={\"size\": 10}, ax=ax)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "# Highlight m_jj row and column\n",
    "for i, label in enumerate(corr_df.columns):\n",
    "    if label == \"m_jj\":\n",
    "        ax.add_patch(plt.Rectangle((i, 0), 1, len(corr_df), fill=False, edgecolor=\"black\", lw=2))\n",
    "        ax.add_patch(plt.Rectangle((0, i), len(corr_df), 1, fill=False, edgecolor=\"black\", lw=2))\n",
    "        break\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.savefig(\"corr_mtrx.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499d657",
   "metadata": {},
   "source": [
    "The threshold we will be using will he of 0.10. Note that tighter cuts were tested during development, but they made no appreciable difference on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_THRESHOLDS = 0.10\n",
    "input_vars, (fig, ax) = get_uncorr_vars(df.corr(), \"m_jj\", CORR_THRESHOLDS, plot=True)\n",
    "ax.tick_params(axis=\"x\", rotation=45)\n",
    "ax.axhline(CORR_THRESHOLDS, color=\"red\", linestyle=\"--\", label=f\"Threshold = {CORR_THRESHOLDS}\")\n",
    "ax.grid(axis=\"y\")\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Correlation with $m_{jj}$\")\n",
    "ax.set_title(\"Variables Ranked by Correlation with $m_{jj}$\")\n",
    "# save to pdf\n",
    "plt.savefig(\"corr_bars.pdf\")\n",
    "plt.show()\n",
    "input_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea1fb1",
   "metadata": {},
   "source": [
    "## Setting up weakly-supervised analysis & Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25101141",
   "metadata": {},
   "source": [
    "We first define the signal and sideband regions using the function `make_regions`. Its important that the signal regions have a full coverage of the di-jet mass distribution, so we also plot the mass hypotheses as well as the border of each signal region to visually ensure that that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b72102",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkgregion_width = 100\n",
    "sigregion_width = 200\n",
    "m0_interval = 100\n",
    "m0_min = 2850\n",
    "m0_max = 5000\n",
    "m0s = np.arange(m0_min, m0_max + 1, m0_interval)\n",
    "\n",
    "regions = make_regions(m0s, sigregion_width, bkgregion_width)\n",
    "\n",
    "# Plot all m0 as vertical lines\n",
    "for region in regions:\n",
    "    plt.axvline(region[\"m0\"], color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.axvline(region[\"sigregion\"][1], color=\"red\", linestyle=\"-\", alpha=0.7)\n",
    "    plt.axvline(region[\"sigregion\"][0], color=\"blue\", linestyle=\"-\", alpha=0.7)\n",
    "    plt.text(region[\"m0\"], 50, f\"{region['m0']}\", rotation=90, verticalalignment='bottom', horizontalalignment='right', fontsize=8)\n",
    "    plt.axvspan(region[\"bkgregion_left\"][0], region[\"bkgregion_left\"][1], color='orange', alpha=0.3)\n",
    "    plt.axvspan(region[\"sigregion\"][0], region[\"sigregion\"][1], color='blue', alpha=0.3)\n",
    "    plt.axvspan(region[\"bkgregion_right\"][0], region[\"bkgregion_right\"][1], color='orange', alpha=0.3)\n",
    "plt.hist(df[\"m_jj\"], bins=100, range=(2500, 6800))\n",
    "plt.xlabel(r\"$m_{jj}$\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Dijet Mass with Signal and Background Regions\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mass hypotheses (m0s) used (GeV):\")\n",
    "print(m0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0eb1e",
   "metadata": {},
   "source": [
    "We start with just two features, namely `tau3j1` and `tau2j1`, and then move on to use a correlation threshold of 0.1 to include more features that are not significantly correlated with the di-jet invariant mass later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe52b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = [\"tau3j1\", \"tau2j1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921b49d",
   "metadata": {},
   "source": [
    "We choose an arbitrary region to run the training workflow. For this particular region, we construct the training, validation and testing datasets using the utility function `dataset_generator`. This function takes only events from the di-jet mass window to construct these datasets. It also takes care of assigning the proper labels: 0 to di-jets in the sidebands, and 1 to di-jets in the hypothesis signal region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1993503",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "region = regions[1]\n",
    "\n",
    "# Data preprocessing happens in dataset_generator\n",
    "train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2989a",
   "metadata": {},
   "source": [
    "## Defining model & training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31544c",
   "metadata": {},
   "source": [
    "We define a simple MLP class that uses `ReLU` activation function, as well as optional dropout, and outputs a single value which represents the predicted probability of an input being in the signal region. Additionally, we define a training function which uses a binary cross-entropy loss with the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 32\n",
    "HIDDEN_LAYERS = 3\n",
    "DROP_RATE = 0\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=HIDDEN_DIM, hidden_layers = HIDDEN_LAYERS, dropout_rate=DROP_RATE):\n",
    "        super().__init__()\n",
    "        layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(input_dim, hidden_dim), \n",
    "                nn.ReLU(), \n",
    "                nn.Dropout(dropout_rate)\n",
    "            ]\n",
    "        )\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        layers.append(nn.Sigmoid()) # output is 1-dimensional for binary classification\n",
    "        layers = nn.ModuleList(layers)\n",
    "        self.MLP = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.MLP(x)\n",
    "    def count_trainable_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "LR = 0.01\n",
    "\n",
    "def train(model, train_loader, val_loader, n_epochs=N_EPOCHS, lr=LR):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    ### Initialize the model, optimizer, and loss function -- what should the input dimensionality be?\n",
    "    model = model.to(device) # move onto the GPU, if present\n",
    "    # loss_fn = nn.BCEWithLogitsLoss() # BCE = Binary Cross Entropy\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    liveloss = PlotLosses(figsize=(9, 4)) \n",
    "    logs = {\"loss\": [], \"val_loss\": []}\n",
    "\n",
    "    ### Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        ### train\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        # Iterate over the training DataLoader\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # Zero the gradients because by default they accumulate because\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ### Move the batch onto the GPU, if present\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(x_batch)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(out, y_batch)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update the weights after the gradients have been computed for this batch\n",
    "            optimizer.step()\n",
    "            # Accumulate the training loss\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Average training loss over all batches in this epoch\n",
    "        train_loss_per_batch = total_train_loss / len(train_loader)\n",
    "        logs['loss'].append(train_loss_per_batch)\n",
    "\n",
    "        ### validate\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        # Iterate over the validation DataLoader\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            \n",
    "            ### Move the batch onto the GPU, if present\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(x_batch)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(out, y_batch)\n",
    "            # Accumulate the validation loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "        # Average validation loss over all validation batches\n",
    "        val_loss_per_batch = total_val_loss / len(val_loader)\n",
    "        # Log the validation loss\n",
    "        logs['val_loss'].append(val_loss_per_batch)\n",
    "\n",
    "        # Update the liveloss plot\n",
    "        liveloss.update(\n",
    "            {\n",
    "                \"loss\": logs[\"loss\"][-1],\n",
    "                \"val_loss\": logs[\"val_loss\"][-1]\n",
    "            }\n",
    "        )\n",
    "        liveloss.send()\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    return model, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba8a8c",
   "metadata": {},
   "source": [
    "We now run the training of the model (with 3 hidden layers, each with dimension 32) for 20 epochs and a learning rate of 0.01 on the previously chosen mass window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_deterministic(seed=SEED)\n",
    "model = MLP(input_dim=len(input_vars), hidden_dim=HIDDEN_DIM, hidden_layers=HIDDEN_LAYERS)\n",
    "print(\"Number of trainable parameters:\", model.count_trainable_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7bce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, logs = train(model, train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20852389",
   "metadata": {},
   "source": [
    "A look at the loss curves shows that the model did not seem to be able to effectively minimize the loss and thus was not able to learn to differentiate di-jets from the signal region from those from the side-bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_logs(logs, region)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d6c0c",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c665a5",
   "metadata": {},
   "source": [
    "From the loss curves seen before above, its clear that the model was not able to effectively learn how to discriminate between di-jets in the signal region and those in the side-bands. We evaluate the trained model on the testing dataset in order to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "with torch.no_grad():\n",
    "    X_test_tensor, y_val_tensor = dss[\"test\"].tensors\n",
    "    y_test_preds = model(X_test_tensor.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3d2d9",
   "metadata": {},
   "source": [
    "If we plot the output distribution, we can see that the output score of all di-jets is ~0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe84c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test_preds.cpu().numpy(), bins=100)\n",
    "plt.xlabel(\"Model Output Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Model Output Scores on Test Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf4777",
   "metadata": {},
   "source": [
    "Moreover, if we plot the ROC curve, its clear that the model performs no better than random choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d61465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making ROC plot\n",
    "fig, ax = plot_roc_curve(dfs, dss, model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9514fb1",
   "metadata": {},
   "source": [
    "### Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f0b84",
   "metadata": {},
   "source": [
    "With the workflow established, we can now run the training and evaluation for all mass hypotheses and collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rslts = []\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    set_deterministic(seed=SEED)\n",
    "\n",
    "    train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MLP(input_dim=len(input_vars), hidden_dim=HIDDEN_DIM, hidden_layers=HIDDEN_LAYERS)\n",
    "\n",
    "    if len(dfs[\"train\"]) / model.count_trainable_params() <= 10:\n",
    "        print(\"Not enough training data for the number of parameters in the model.\")\n",
    "        print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")\n",
    "        break\n",
    "\n",
    "    model, logs = train(model, n_epochs=N_EPOCHS, lr=LR, train_loader=train_loader, val_loader=val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_preds)\n",
    "\n",
    "    rslts.append(\n",
    "        {\n",
    "            \"region\": region,\n",
    "            \"model\": model,\n",
    "            \"logs\": logs,\n",
    "            \"roc\": (fpr, tpr, thresholds),\n",
    "            \"auc\": auc(fpr, tpr),\n",
    "            \"X_test_tensor\": X_test_tensor,\n",
    "            \"y_test_tensor\": y_test_tensor,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rslts_grids(rslts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a16abb",
   "metadata": {},
   "source": [
    "These results show no evidence of the model having been able to discriminate between signal region and side-band region di-jets better than random choice, and thus we have not found the signal we are searching for yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c7a5a",
   "metadata": {},
   "source": [
    "## Including more features\n",
    "\n",
    "We now include only those features that pass a correlation cut of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_THRESHOLDS = 0.1\n",
    "input_vars, _ = get_uncorr_vars(df.corr(), \"m_jj\", CORR_THRESHOLDS, plot=False)\n",
    "input_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c796c",
   "metadata": {},
   "source": [
    "### Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a7220",
   "metadata": {},
   "source": [
    "Now that the workflow is setup, we perform a scan over the entire di-jet mass range. We use the same di-jet mass regions defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1886bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rslts = []\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    set_deterministic(seed=SEED)\n",
    "\n",
    "    train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MLP(input_dim=len(input_vars), hidden_dim=HIDDEN_DIM, hidden_layers=HIDDEN_LAYERS)\n",
    "\n",
    "    if len(dfs[\"train\"]) / model.count_trainable_params() <= 10:\n",
    "        print(\"Not enough training data for the number of parameters in the model.\")\n",
    "        print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")\n",
    "        break\n",
    "\n",
    "    model, logs = train(model, n_epochs=N_EPOCHS, lr=LR, train_loader=train_loader, val_loader=val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_preds)\n",
    "\n",
    "    rslts.append(\n",
    "        {\n",
    "            \"region\": region,\n",
    "            \"model\": model,\n",
    "            \"logs\": logs,\n",
    "            \"roc\": (fpr, tpr, thresholds),\n",
    "            \"auc\": auc(fpr, tpr),\n",
    "            \"X_test_tensor\": X_test_tensor,\n",
    "            \"y_test_tensor\": y_test_tensor,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rslts_grids(rslts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209efe66",
   "metadata": {},
   "source": [
    "## Additional scans with other hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa337a9",
   "metadata": {},
   "source": [
    "The above results clearly show that the classifier mostly performed around the same as random chance with very little deviation from this. We now do an additional scan over the same regions, but with a larger model. We do this by having 4 hidden layers instead of 3. Note that this limits the reach of the search, as the current implementation only goes up to the mass hypothesis window for which there are enough training events given a certain number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rslts = []\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    set_deterministic(seed=SEED)\n",
    "\n",
    "    train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MLP(input_dim=len(input_vars), hidden_dim=HIDDEN_DIM, hidden_layers=4)\n",
    "\n",
    "    if len(dfs[\"train\"]) / model.count_trainable_params() <= 10:\n",
    "        print(\"Not enough training data for the number of parameters in the model.\")\n",
    "        print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")\n",
    "        break\n",
    "\n",
    "    model, logs = train(model, n_epochs=N_EPOCHS, lr=LR, train_loader=train_loader, val_loader=val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_preds)\n",
    "\n",
    "    rslts.append(\n",
    "        {\n",
    "            \"region\": region,\n",
    "            \"model\": model,\n",
    "            \"logs\": logs,\n",
    "            \"roc\": (fpr, tpr, thresholds),\n",
    "            \"auc\": auc(fpr, tpr),\n",
    "            \"X_test_tensor\": X_test_tensor,\n",
    "            \"y_test_tensor\": y_test_tensor,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdac080",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rslts_grids(rslts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7074a7da",
   "metadata": {},
   "source": [
    "Unfortunately, the previously observed null results still hold for this new and slightly larger model. Adding more model trainable parameters by using a larger model would limit the range of the di-jet mass range we could scan, unless a different binning strategy was adopted for the region definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b25420",
   "metadata": {},
   "source": [
    "### Smaller model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bddbb8",
   "metadata": {},
   "source": [
    "We now try a smaller model, which might help with any overfitting that might be happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rslts = []\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    set_deterministic(seed=SEED)\n",
    "\n",
    "    train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MLP(input_dim=len(input_vars), hidden_dim=16, hidden_layers=HIDDEN_DIM)\n",
    "\n",
    "    if len(dfs[\"train\"]) / model.count_trainable_params() <= 10:\n",
    "        print(\"Not enough training data for the number of parameters in the model.\")\n",
    "        print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")\n",
    "        break\n",
    "\n",
    "    model, logs = train(model, n_epochs=N_EPOCHS, lr=LR, train_loader=train_loader, val_loader=val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_preds)\n",
    "\n",
    "    rslts.append(\n",
    "        {\n",
    "            \"region\": region,\n",
    "            \"model\": model,\n",
    "            \"logs\": logs,\n",
    "            \"roc\": (fpr, tpr, thresholds),\n",
    "            \"auc\": auc(fpr, tpr),\n",
    "            \"X_test_tensor\": X_test_tensor,\n",
    "            \"y_test_tensor\": y_test_tensor,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rslts_grids(rslts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27b824",
   "metadata": {},
   "source": [
    "### Wider sidebands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31598d20",
   "metadata": {},
   "source": [
    "Another approach is to widen the side-bands. The logic behind this is that, if the bump in the $m_{jj}$ distribution is around the same size as the mass window, the model might not be able to properly learn what is background and what is signal. Thus, by extending the side-bands, we allow more events that are outside of this potentially wide bump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bkgregion_width = 200\n",
    "sigregion_width = 200\n",
    "m0_interval = 100\n",
    "m0_min = 2850\n",
    "m0_max = 5000\n",
    "m0s = np.arange(m0_min, m0_max + 1, m0_interval)\n",
    "\n",
    "regions = make_regions(m0s, sigregion_width, bkgregion_width)\n",
    "\n",
    "# Plot all m0 as vertical lines\n",
    "for region in regions:\n",
    "    plt.axvline(region[\"m0\"], color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(region[\"sigregion\"][1], color='red', linestyle='-', alpha=0.7)\n",
    "    plt.axvline(region[\"sigregion\"][0], color='blue', linestyle='-', alpha=0.7)\n",
    "    plt.text(region[\"m0\"], 50, f\"{region['m0']}\", rotation=90, verticalalignment='bottom', horizontalalignment='right', fontsize=8)\n",
    "    plt.axvspan(region[\"bkgregion_left\"][0], region[\"bkgregion_left\"][1], color='orange', alpha=0.3)\n",
    "    plt.axvspan(region[\"sigregion\"][0], region[\"sigregion\"][1], color='blue', alpha=0.3)\n",
    "    plt.axvspan(region[\"bkgregion_right\"][0], region[\"bkgregion_right\"][1], color='orange', alpha=0.3)\n",
    "plt.hist(df[\"m_jj\"], bins=100, range=(2500, 6800))\n",
    "plt.xlabel(r\"$m_{jj}$\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Dijet Mass with Signal and Background Regions\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mass hypotheses (m0s) used (GeV):\")\n",
    "print(m0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rslts = []\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    set_deterministic(seed=SEED)\n",
    "\n",
    "    train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MLP(input_dim=len(input_vars), hidden_dim=HIDDEN_DIM, hidden_layers=4)\n",
    "\n",
    "    if len(dfs[\"train\"]) / model.count_trainable_params() <= 10:\n",
    "        print(\"Not enough training data for the number of parameters in the model.\")\n",
    "        print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")\n",
    "        break\n",
    "\n",
    "    model, logs = train(model, n_epochs=N_EPOCHS, lr=LR, train_loader=train_loader, val_loader=val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_preds)\n",
    "\n",
    "    rslts.append(\n",
    "        {\n",
    "            \"region\": region,\n",
    "            \"model\": model,\n",
    "            \"logs\": logs,\n",
    "            \"roc\": (fpr, tpr, thresholds),\n",
    "            \"auc\": auc(fpr, tpr),\n",
    "            \"X_test_tensor\": X_test_tensor,\n",
    "            \"y_test_tensor\": y_test_tensor,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rslts_grids(rslts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b143950",
   "metadata": {},
   "source": [
    "## Wider signal region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da800bf3",
   "metadata": {},
   "source": [
    "This approach follows a similar logic as the previous experiment where we widened the side-bands. By widening the signal region, more of the bump will be labeled signal and the model might be able to learn to differentiate signal from background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801977b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bkgregion_width = 100\n",
    "sigregion_width = 300\n",
    "m0_interval = 100\n",
    "m0_min = 2850\n",
    "m0_max = 5000\n",
    "m0s = np.arange(m0_min, m0_max + 1, m0_interval)\n",
    "\n",
    "regions = make_regions(m0s, sigregion_width, bkgregion_width)\n",
    "\n",
    "# Plot all m0 as vertical lines\n",
    "for region in regions:\n",
    "    plt.axvline(region[\"m0\"], color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(region[\"sigregion\"][1], color='red', linestyle='-', alpha=0.7)\n",
    "    plt.axvline(region[\"sigregion\"][0], color='blue', linestyle='-', alpha=0.7)\n",
    "    plt.text(region[\"m0\"], 50, f\"{region['m0']}\", rotation=90, verticalalignment='bottom', horizontalalignment='right', fontsize=8)\n",
    "    plt.axvspan(region[\"bkgregion_left\"][0], region[\"bkgregion_left\"][1], color='orange', alpha=0.3)\n",
    "    plt.axvspan(region[\"sigregion\"][0], region[\"sigregion\"][1], color='blue', alpha=0.3)\n",
    "    plt.axvspan(region[\"bkgregion_right\"][0], region[\"bkgregion_right\"][1], color='orange', alpha=0.3)\n",
    "plt.hist(df[\"m_jj\"], bins=100, range=(2500, 6800))\n",
    "plt.xlabel(r\"$m_{jj}$\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Dijet Mass with Signal and Background Regions\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Mass hypotheses (m0s) used (GeV):\")\n",
    "print(m0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36209a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rslts = []\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "    set_deterministic(seed=SEED)\n",
    "\n",
    "    train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MLP(input_dim=len(input_vars), hidden_dim=HIDDEN_DIM, hidden_layers=4)\n",
    "\n",
    "    if len(dfs[\"train\"]) / model.count_trainable_params() <= 10:\n",
    "        print(\"Not enough training data for the number of parameters in the model.\")\n",
    "        print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")\n",
    "        break\n",
    "\n",
    "    model, logs = train(model, n_epochs=N_EPOCHS, lr=LR, train_loader=train_loader, val_loader=val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_preds)\n",
    "\n",
    "    rslts.append(\n",
    "        {\n",
    "            \"region\": region,\n",
    "            \"model\": model,\n",
    "            \"logs\": logs,\n",
    "            \"roc\": (fpr, tpr, thresholds),\n",
    "            \"auc\": auc(fpr, tpr),\n",
    "            \"X_test_tensor\": X_test_tensor,\n",
    "            \"y_test_tensor\": y_test_tensor,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa39afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rslts_grids(rslts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2578fc",
   "metadata": {},
   "source": [
    "For this particular approach, the training of the model for $m_{jj} = 3450 \\text{GeV}$ does too much over training as can be seen for the other mass hypotheses. In fact, motivated by this observation, if one compares the AUC vs. $m_0$ plot between all of the difference approaches taken, one can see that models trained on and around this mass hypothesis tend yield a slightly higher AUC. While this is not conclusive evidence that there is something there, it would be something worth investigating further in future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf7959",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These results show no sign of detection of the bump in the di-jet mass distribution. The work done so far has consisted of a simplified CWoLa approach, which means that the obvious next steps in this search would be a more careful and rigurous implementation of this technique. This could involve a stronger decorellation method, a more careful binning strategy for the mass window generation, the implementation of a cross-validation strategy, etc. These approaches have the potential to increase our sensitivity to the resonance that we were searching for, providing us to have a better chance of finding the mass bump, and to investigate potential signs of it such as the one observed for $m_{jj} = 3450 \\text{ GeV}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a85a8",
   "metadata": {},
   "source": [
    "## EXTRA: Signal detection? (Probably not...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5429e67",
   "metadata": {},
   "source": [
    "The following preliminary results were obtained in the process of developing this notebook and is kept here as it serves a the only clear sign of a potential signal. Unfortunately, it's highly likely that this result is a false positive, as the initial implementation of the `make_regions` function had an issue where it would make regions too wide, which might have resulted in the window spilling over into the peak of the $m_jj$ distribution. This is an issue since one of the requirements of the applied technique is that the distribution be smooth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bkgregion_width = 100\n",
    "sigregion_width = 200\n",
    "m0_min = 2800\n",
    "m0_max = 3200\n",
    "m0_interval = 150\n",
    "m0s = np.arange(m0_min, m0_max + 1, m0_interval)\n",
    "\n",
    "regions = make_regions(m0s, sigregion_width, bkgregion_width)\n",
    "\n",
    "# Plot all m0 as vertical lines\n",
    "for region in regions:\n",
    "    plt.axvline(region[\"m0\"], color='gray', linestyle='--', alpha=0.5)\n",
    "    # Red right line for signal region\n",
    "    plt.axvline(region[\"sigregion\"][1], color='red', linestyle='-', alpha=0.7)\n",
    "    # blue left line for signal region\n",
    "    plt.axvline(region[\"sigregion\"][0], color='blue', linestyle='-', alpha=0.7)\n",
    "    plt.axvline(region[\"bkgregion_left\"][0], color='green', linestyle='-', alpha=0.7)\n",
    "    plt.axvline(region[\"bkgregion_right\"][1], color='green', linestyle='-', alpha=0.7)\n",
    "    plt.text(region[\"m0\"], 50, f\"{region['m0']}\", rotation=90, verticalalignment='bottom', horizontalalignment='right', fontsize=8)\n",
    "plt.hist(df[\"m_jj\"], bins=100, range=(m0_min - sigregion_width, m0_max + sigregion_width))\n",
    "plt.xlabel(r\"$m_{jj}$\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Dijet Mass with Signal and Background Regions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a10d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainining hyperparameters\n",
    "N_EPOCHS = 25\n",
    "HIDDEN_DIM = 16\n",
    "HIDDEN_LAYERS = 3\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.01\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rslts_2 = []\n",
    "\n",
    "for region in regions:\n",
    "    print(region)\n",
    "\n",
    "    train_loader, val_loader, test_loader, dfs, dss = dataset_generator(df, input_vars, region, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = MLP(input_dim=len(input_vars), hidden_dim=HIDDEN_DIM, hidden_layers=HIDDEN_LAYERS)\n",
    "\n",
    "    if len(dfs[\"train\"]) / model.count_trainable_params() <= 10:\n",
    "        print(\"Not enough training data for the number of parameters in the model.\")\n",
    "        print(f\"Train size: {len(dfs['train'])}, Val size: {len(dfs['val'])}, Test size: {len(dfs['test'])}\")\n",
    "        break\n",
    "\n",
    "    model, logs = train(model, n_epochs=N_EPOCHS, lr=LR, train_loader=train_loader, val_loader=val_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor, y_test_tensor = dss[\"test\"].tensors\n",
    "        y_test_preds = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tensor, y_test_preds)\n",
    "\n",
    "    rslts_2.append(\n",
    "        {\n",
    "            \"region\": region,\n",
    "            \"model\": model,\n",
    "            \"logs\": logs,\n",
    "            \"roc\": (fpr, tpr, thresholds),\n",
    "            \"auc\": auc(fpr, tpr),\n",
    "            \"X_test_tensor\": X_test_tensor,\n",
    "            \"y_test_tensor\": y_test_tensor,\n",
    "            \"dfs\": dfs,\n",
    "            \"dss\": dss\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make grid of roc curves from rslts. With 3 columns\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(rslts_2) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "for i, rslt in enumerate(rslts_2):\n",
    "    fpr, tpr, thresholds = rslt[\"roc\"]\n",
    "    ax = axes[i]\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % rslt[\"auc\"])\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f\"ROC for $m_0$ = {rslt['region']['m0']} GeV\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# Make grid of loss plots from rslts. With 3 columns\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(rslts_2) / n_cols))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "for i, rslt in enumerate(rslts_2):\n",
    "    ax = axes[i]\n",
    "    ax.plot(rslt[\"logs\"][\"loss\"], label=\"Training Loss\")\n",
    "    ax.plot(rslt[\"logs\"][\"val_loss\"], label=\"Validation Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Training and Validation Loss ($m_0$ = {rslt['region']['m0']} GeV)\")\n",
    "    ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107868ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC vs mass hypothesis\n",
    "m0s = [rslt[\"region\"][\"m0\"] for rslt in rslts_2]\n",
    "aucs = [rslt[\"auc\"] for rslt in rslts_2]\n",
    "plt.plot(m0s, aucs, marker='o')\n",
    "plt.xlabel(r\"$m_0$ [GeV]\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"AUC vs Mass Hypothesis\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0083766",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD=0.6\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_scores = rslts_2[0][\"model\"](rslts_2[0][\"X_test_tensor\"].to(device)).cpu().numpy()\n",
    "plt.hist(test_scores, bins=100, range=(0,1))\n",
    "plt.xlabel(\"Model Output Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Model Output Scores on Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dijet mass histogram for events passing threshold along with all events dijet mass\n",
    "df_test_pass = rslts_2[0][\"dfs\"][\"test\"][test_scores > THRESHOLD]\n",
    "plt.hist(df_test_pass[\"m_jj\"], bins=100, range=(2500, 6500), alpha=0.5, label=\"Passed\", density=True)\n",
    "plt.hist(rslts_2[0][\"dfs\"][\"test\"][\"m_jj\"], bins=100, range=(2500, 6500), alpha=0.5, label=\"All\", density=True)\n",
    "plt.xlabel(r\"$m_{jj}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying model to full dataset for one of the mass points\n",
    "test_input_data = dataset_generator(df, input_vars, regions[1], batch_size=BATCH_SIZE)\n",
    "with torch.no_grad():\n",
    "    preds = rslts_2[1][\"model\"](\n",
    "        test_input_data[4][\"test\"].tensors[0].to(device)\n",
    "    ).cpu().numpy()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8450ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_input_data[3][\"test\"]\n",
    "test_df[\"model_score\"] = preds\n",
    "# Plot model score distribution\n",
    "plt.hist(test_df[\"model_score\"], bins=100, range=(0,1))\n",
    "plt.xlabel(\"Model Output Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Model Output Scores on Test Set\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
